---
title: "Thinking Out Loud on Career Choice"
date: 2023-03-17
---
I'm not sure if I'm a good fit for doing anything with respect to climate change.  I am looking at switching to, 
e.g. AI, or, if not that, then some other EA-aligned thing.
Question 1: Why am I thinking of doing this? Or why do I not really give an ass about what I study now?
Question 2: Why would it be unwise to do this
Question 3: Is my current decision-making algorithm stupid, and if so, how to repair?

Question 1:

I've been studying SET at TU Delft for two years now - I've been reasonably involved in the climate / energy space, enough to get a feel for things.
The primary problem, more important than the rest, is that I don't see a path to impact that uses my comparative advantage.
I've seen friends who have an impressive grasp on analysing tech from the "engineer's perspective" that I thought I could have.
Turns out, this is a skill that I don't, by default, have.  Now I presumably could develop this skill - if I were to, what then?
Do I have any especial insights into energy policy?  None that aren't derived from a paper I've read, or a podcast I've listened to;
furthermore, most of my opinions in this space are opinions for the sake of having them - they're frankly not epistemically honest, and I couldn't make
them be so without a lot of work.

Further, it seems like, if any career is to be high-impact, it would be something like policy - here I may have a relative disadvantage due to my
lack of empathy with other people.  If I were to go the policy-route, I don't know that I could produce anything of value.
On the other hand, the academic route seems pointless too - if technology is already so highly developed that it's macroscale 
predictable (like solar learning-curves), what could I possibly have to add?  Presumably all I could do is step into the middle of the 
process and just help others push the boulder up....

Then, there's "politics", but I think the appeal of this to me is the appeal of being a character people could elect - not even power, but self-image.
While it would be 'cute' to become a congressman, would I be willing to sacrifice for it?  Probably not much.  Would I bring anything unique to the table?
Maybe, but to the extent that I do, it would be just smuggling in other peoples' ideas to political respectability; and to the extent that politicians
are ostensibly bound by their constituents, it doesn't seem like I'd have the latitude to act as I see fit.

And it's worth mentioning, but I think not serious, that I don't see any of these as fun - I mention this only lightly, because I believe that I could
plausibly change this by becoming really invested in something and working hard on it - then it might be.  But on the other hand, we come to question 2:

Question 2:

On what grounds am I considering making this change - what is 'this change' and why?  For the sake of concreteness, let's say I want to become an "AI Safety Researcher".  It's possible it's something else like bio-safety stuff or some other EA-aligned thing or even something else entirely.  What good seems like it would come from this?  For one, I really enjoy the EA/LW community to a degree that just isn't true of anything else.  It's a group of people that natively speak my language without an accent, often better than I do, and I love it.  For another, the feeling of an ethical impulse I can't seem to adequately put off - realistically I essentially believe that they're right about biosafety and AI safety (though I'd have to do some research to confirm, if this were to be a decision-crux).  Further, it seems that, e.g., and especially AI safety is something that I find fundamentally interesting - 
maybe my actual intellectual calling.  I'm scared as to whether I'd be good at it - could I do work of quality similar to e.g. Byrnes or TurnTrout, etc.?  I think the answer is maybe - I might, especially if I work hard, be able to do really well; maybe not.  Remember in fact that my whole plan for QMAI, and Delft in general was based on an idiot version of "recognising phase transitions sounds a lot like intelligence" - that, whatever story I've passed around, that is basically why I went, however utterly misguided that turned out to be.  Studying Intelligence was what you wanted all-along - maybe not so much the alignment stuff, but they're damn closely related.  If I were to 'flip a coin' I'd know what I'm rooting for as soon as the coin is in the air.  But how consistent is this feeling?

Question 3:
Why then am I afraid to make this decision if it seems so one-sided?  That's the point - it shouldn't seem one-sided.  In particular, I think the decision-algorithm I'm using now is totally non-replicable - I know that my day-to-day opinions and feelings vary a lot, maybe a lot more than other people.  Point two for being concerned about the decision-algorithm is that I've been here before - I did the same thing moving into climate.  This suggests that I just don't have the right algorithm to make these decisions reliably and well.  Point three is this "do I want to sell my soul to EA" thing - my memetic independence from this movement is a) a source of immense pride and I think likely to be the source of a lot of development in my character, and b) tenuous at best.  On the other hand, speaking truthfully, I think that much of the hesitation comes from imagining a particular friend laughing a little bit at me - someone who I know has a better introsepctive, or at least planning-ahead, capacity than me.  If I were in the world where this really is just 'the grass is greener', I can say almoost for certain that the decision-algorithm wouldn't detect this, and I'd be bouncing around ad infinitum.  Are there some object-level things that I think are different in the climate-v-AI vs. Physics-v-climate?  At least maybe, but not clear if this is short-sightedness or what.

Question 4:
What can I change about my decision-algorithm that would make it more robust?  That seems a smarter angle to take than just asking what I should do as the 'right answer'.  It's clear that one part of this is a skill of, let's say, 'epistemic independence' - a more critical-thinking view of everything I read - changing myself enough that studying a language doesn't sound like the greatest thing in the world (it does because it's stimulating and intellectual and signals intelligence, but requires just a ton of memorisation, and no thought - very 'kata' like), compared to, e.g. writing a blog post or some kind of critique.  Need to become a person who asks questions (and tries to answer them) rather than looks to be told answers.

Another part of it is that I need to do some more research at an object-level - if I were in the world where EA is AI nuts, my algorithm wouldn't know it now.  Same goes, as well, for climate change to be honest.  I need a big ol' dose of intellectual honesty / epistemic responsibility.

Another thing I need is a good degree of independence (these things are all related, but distinct enough to be worth mentioning separately).  This applies to 'physical' independence, in the sense of being more proactive about handling my bullshit - to be the kind of person who could run a startup if he wanted.  I think 'intellectual' independence is mostly an admixture of the above-mentioned epistemic-responsibility and the 'would I be selling my soul to EA' problem.  The latter I think is at least partially responded to by writing this out and thinking seriously about it...

Answer to question 4:
I think I've known for a while, but it's good to have in writing, that I need some serious character-development.  What should this look like?

One option is going the 'entrepreneurship' route and working the line for a few years.

Another option is making it a 'project' (or maybe it's a meta-project) similar to those I'm engaged in now.  Such a project might be seriously starting a blog, and/or "start from a question and write a research paper getting to the answer" and see if I can figure out how to think for myself.

Are there other options?  I'll weigh these and come back to edit this post, as my current project continues and eventually ends.
